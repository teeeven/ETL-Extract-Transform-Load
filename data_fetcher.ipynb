{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Data Fetcher Script for ETL Pipeline\n",
    "# -------------------------------------------------------------------------------\n",
    "# This script forms the core of an ETL pipeline designed to automate the extraction of data from \n",
    "# multiple external APIs, which includes educational assessments, user engagement, and program \n",
    "# performance metrics. \n",
    "# \n",
    "# This workbook covers the first step of the pipeline, the Extract phase, which involves fetching data from the API:\n",
    "# 1. Pipeline: API Call\n",
    "#    - Purpose: Retrieve data from the API, including information on users, \n",
    "#      groups, risk scores, phishing tests, and training campaigns.\n",
    "#    - Description: This stage uses API endpoints to gather raw data, transforming it \n",
    "#      into a structured format for ingestion.\n",
    "# \n",
    "# The main functionalities of this script include:\n",
    "#\n",
    "# 1. Securely fetching data from diverse API endpoints, each related to specific aspects of educational\n",
    "#    programs and participant activities.\n",
    "# 2. Normalizing and transforming this data into a structured format suitable for analytical purposes,\n",
    "#    ensuring consistency and readiness for SQL database integration.\n",
    "# 3. Preparing the data for merging and further processing by cleaning, renaming, and converting data types\n",
    "#    to ensure optimal alignment with the database schema used in downstream analytics.\n",
    "# 4. Loading the processed data into specific Pandas dataframes, which will further be processed in the Transform step.\n",
    "#    \n",
    "#\n",
    "# The script is built with robust error handling and logging to track the data flow and catch issues early in\n",
    "# the process, enhancing the reliability and maintainability of the data pipeline.\n",
    "# -------------------------------------------------------------------------------\n",
    "# End of Summary\n",
    "# -------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Set up logging configuration, and import libraries\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from requests.exceptions import RequestException\n",
    "from datetime import datetime, timedelta\n",
    "from pandas import json_normalize\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    "    handlers=[\n",
    "        logging.FileHandler(\"api_script.log\"),  \n",
    "        logging.StreamHandler()  \n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"Logging setup initialized with a single file and console output.\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Step 1: Define the API key, headers, and create functions to handle the API request with retry logic for rate limits\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "# Set up headers for API requests\n",
    "headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n",
    "\n",
    "# Function to handle the API request with retry logic for rate limits\n",
    "\n",
    "# Rate Limiting notes from documentation\n",
    "# API is limited to 2,000 requests per day plus the number of licensed users on your account. \n",
    "# The APIs may only be accessed four times per second. The API burst limit is 50 requests per minute. \n",
    "# Please note that the API bursts limits will start around five (5) minutes and the API daily limit starts around twenty-four (24) hours from the first API request.\n",
    "\n",
    "# To track time intervals for rate limiting\n",
    "last_request_time = None\n",
    "requests_in_last_minute = 0\n",
    "minute_start_time = datetime.now()\n",
    "\n",
    "def fetch_data(url, headers, params, max_retries=5):\n",
    "    \"\"\"\n",
    "    Fetch data from a given URL with specified headers and parameters, adhering to rate limits.\n",
    "    This function makes a GET request to the specified URL with the provided headers and parameters.\n",
    "    It enforces rate limits of 4 requests per second and 50 requests per minute. If the rate limit\n",
    "    is exceeded, the function will pause and retry the request. If the server responds with a 429\n",
    "    status code (rate limit exceeded), the function will wait for the specified 'Retry-After' time\n",
    "    before retrying. The function will retry the request up to 'max_retries' times in case of failures.\n",
    "    Args:\n",
    "        url (str): The URL to send the GET request to.\n",
    "        headers (dict): The headers to include in the GET request.\n",
    "        params (dict): The parameters to include in the GET request.\n",
    "        max_retries (int, optional): The maximum number of retries in case of failures. Defaults to 5.\n",
    "    Returns:\n",
    "        dict or None: The JSON response from the server if the request is successful, None otherwise.\n",
    "    Raises:\n",
    "        RequestException: If there is an issue with the request that is not related to rate limiting.\n",
    "    \"\"\"\n",
    "    global last_request_time, requests_in_last_minute, minute_start_time\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        # Check if we need to wait to comply with 4 requests/second and 50 requests/minute limits\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Enforce 50 requests/minute limit\n",
    "        if (current_time - minute_start_time).total_seconds() >= 60:\n",
    "            requests_in_last_minute = 0\n",
    "            minute_start_time = current_time\n",
    "            \n",
    "        # Pause if we exceed 50 requests within the last minute\n",
    "        if requests_in_last_minute >= 50:\n",
    "            sleep_time = 60 - (current_time - minute_start_time).total_seconds()\n",
    "            logging.info(f\"Reached burst limit. Pausing for {sleep_time:.2f} seconds to comply with rate limits.\")\n",
    "            time.sleep(sleep_time)\n",
    "            requests_in_last_minute = 0\n",
    "            minute_start_time = datetime.now()\n",
    "        \n",
    "        # Enforce 4 requests per second\n",
    "        if last_request_time and (current_time - last_request_time).total_seconds() < 0.25:\n",
    "            time.sleep(0.25 - (current_time - last_request_time).total_seconds())\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            last_request_time = datetime.now()\n",
    "            requests_in_last_minute += 1\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                logging.info(\"Data fetched successfully.\")\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:  \n",
    "                retry_after = int(response.headers.get('Retry-After', 5))\n",
    "                logging.warning(f\"Rate limit hit. Retrying after {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "                retries += 1\n",
    "            else:\n",
    "                logging.error(f\"Error {response.status_code}: {response.text}\")\n",
    "                break\n",
    "        except RequestException as e:\n",
    "            logging.error(f\"RequestException: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    logging.error(\"Max retries reached. Failed to fetch data.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to apply date conversion to a list of columns\n",
    "def convert_dates(df, columns):\n",
    "    \"\"\"\n",
    "    Convert specified columns in a DataFrame to datetime format.\n",
    "\n",
    "    This function takes a DataFrame and a list of column names, and converts\n",
    "    each specified column to datetime format. If a column does not exist in \n",
    "    the DataFrame, it is skipped. Any errors during conversion will result in \n",
    "    NaT (Not a Time) values. The timezone information is removed from the \n",
    "    datetime values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the columns to be converted.\n",
    "        columns (list of str): A list of column names to be converted to datetime.\n",
    "\n",
    "    Returns:\n",
    "        None: The function modifies the DataFrame in place.\n",
    "\n",
    "    Logs:\n",
    "        Logs an info message for each column that is successfully converted.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce').dt.tz_localize(None)\n",
    "            logging.info(f\"Converted {col} to datetime.\")\n",
    "\n",
    "def rename_and_cast_columns(df, column_mappings):\n",
    "    \"\"\"\n",
    "    Rename specified columns in a DataFrame and ensure they are cast to int64.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing columns to rename and cast.\n",
    "    column_mappings (dict): A dictionary where keys are current column names and values are the new names.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with renamed columns and appropriate casting.\n",
    "    \"\"\"\n",
    "    # Rename columns\n",
    "    df.rename(columns=column_mappings, inplace=True)\n",
    "\n",
    "    # Ensure columns are of type int64\n",
    "    for new_col in column_mappings.values():\n",
    "        if new_col in df.columns:\n",
    "            df[new_col] = pd.to_numeric(df[new_col], errors='coerce').fillna(0).astype('int64')\n",
    "            logging.info(f\"Column '{new_col}' renamed and cast to int64.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define the helper function to convert ID columns\n",
    "def convert_id_columns_to_int64(dataframes, id_columns):\n",
    "    \"\"\"\n",
    "    Converts specified ID columns in each DataFrame within the provided dictionary to int64.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes (dict): Dictionary of DataFrames to process.\n",
    "    id_columns (list): List of column names to convert to int64 if present in each DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Updated dictionary with ID columns converted to int64 where applicable.\n",
    "    \"\"\"\n",
    "    for table_name, df in dataframes.items():\n",
    "        for col in id_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "                    logging.info(f\"Converted column '{col}' in DataFrame '{table_name}' to int64.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error converting column '{col}' in DataFrame '{table_name}': {e}\")\n",
    "    return dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: API Calls and Data Processing\n",
    "\n",
    "# Log the start of the script\n",
    "today = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "logging.info(f\"API script started: {today}\")\n",
    "\n",
    "\n",
    "# -------- API 1: Data Retrieval Endpoint --------\n",
    "url_data = 'https://api.example.com/v99/data_history'\n",
    "params_data = {\n",
    "    'include_all': 'true'  # Retrieve comprehensive historical data. Defaults to six months if not specified.\n",
    "}\n",
    "\n",
    "data_history = fetch_data(url_data, headers, params_data)\n",
    "if data_history:\n",
    "    df_data_history = pd.json_normalize(data_history, sep='_')\n",
    "    # Convert dates to datetime\n",
    "    date_cols = ['date']\n",
    "    convert_dates(df_data_history, date_cols)\n",
    "\n",
    "    # Rename date to date_recorded for insertion into SQL database\n",
    "    df_data_history.rename(columns={'date': 'date_recorded'}, inplace=True)\n",
    "    \n",
    "    logging.info(\"Data History DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 1.\")\n",
    "else:\n",
    "    blank_df_data_history = pd.DataFrame()\n",
    "    logging.warning(\"No data history fetched.\")\n",
    "\n",
    "\n",
    "\n",
    "# -------- API 2: Get a List of All Users Endpoint --------\n",
    "url_users = 'https://api.example.com/v99/user_data'\n",
    "params_users = {\n",
    "    'status': 'active',  # Returns a list of all active users.\n",
    "    'expand': 'details',   # Expands additional details related to each user.\n",
    "    'per_page': 500  # Number of records per page (maximum allowed).\n",
    "}\n",
    "\n",
    "user_list = fetch_data(url_users, headers, params_users)\n",
    "if user_list:\n",
    "    df_user_list = pd.json_normalize(user_list, sep='_')\n",
    "\n",
    "    # Create full name column\n",
    "    df_user_list['full_name'] = df_user_list['first_name'] + ' ' + df_user_list['last_name']\n",
    "\n",
    "    # Clean up department names\n",
    "    df_user_list['department'] = df_user_list['department'].str.replace('_', ' ')\n",
    "\n",
    "    # Convert dates to datetime\n",
    "    date_cols = ['last_sign_in', 'employee_start_date', 'archived_at', 'custom_date_1', 'custom_date_2']\n",
    "    convert_dates(df_user_list, date_cols)\n",
    "\n",
    "    # Rename columns for appropriate ID references\n",
    "    df_user_list.rename(columns={'id': 'user_id', 'employee_number': 'internal_id'}, inplace=True)\n",
    "\n",
    "    logging.info(\"Users DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 2.\")\n",
    "\n",
    "else:\n",
    "    blank_df_user_list = pd.DataFrame()\n",
    "    logging.warning(\"No user data fetched.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------- API 3: Get a Specific User's Historical Data Endpoint --------\n",
    "# This will be done in a loop for each user, fetching their historical data separately as one API call per user.\n",
    "# Method: Loop through each unique user ID, fetch their historical data, and store it in a list of DataFrames.\n",
    "\n",
    "# Define unique user IDs from the dataframe\n",
    "user_ids = df_user_list['user_id'].unique()  \n",
    "params_user_history = {\n",
    "    'complete_history': 'true'  # Include the entire historical data of the user\n",
    "}\n",
    "\n",
    "# List to store individual user historical data\n",
    "user_history_dfs = []\n",
    "\n",
    "for user in user_ids:\n",
    "    # Construct URL specific to each user\n",
    "    url_user_history = f'https://api.example.com/v99/users/{user}/history'\n",
    "    try:\n",
    "        # Make API call for each user\n",
    "        user_history_data = fetch_data(url_user_history, headers, params_user_history)\n",
    "        \n",
    "        # Process the data if it's available\n",
    "        if user_history_data:\n",
    "            df_user_history = pd.json_normalize(user_history_data, sep='_')\n",
    "            \n",
    "            # Convert dates to datetime if 'date' column exists\n",
    "            date_cols = ['date']\n",
    "            convert_dates(df_user_history, date_cols)\n",
    "\n",
    "            # Add 'user_id' column to enable proper merging\n",
    "            df_user_history['user_id'] = user\n",
    "            \n",
    "            # Append processed DataFrame to the list\n",
    "            user_history_dfs.append(df_user_history)\n",
    "        else:\n",
    "            logging.warning(f\"No historical data fetched for user ID: {user}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for user ID {user}: {e}\")\n",
    "\n",
    "# Concatenate all user historical data DataFrames, if any\n",
    "if user_history_dfs:\n",
    "    combined_user_history_df = pd.concat(user_history_dfs, ignore_index=True)\n",
    "    \n",
    "    # Select only the necessary columns from the user list DataFrame for the merge\n",
    "    df_users_subset = df_user_list[['user_id', 'internal_id', 'first_name', 'last_name', \n",
    "                                    'full_name', 'department', 'job_title']]\n",
    "    \n",
    "    # Merge the combined history data with the subset of the original users listing DataFrame\n",
    "    combined_user_history_df = pd.merge(combined_user_history_df, df_users_subset, \n",
    "                                              how='left', on='user_id')\n",
    "\n",
    "    logging.info(\"Combined Historical Data DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 3.\")\n",
    "else:\n",
    "    blank_df_combined_history = pd.DataFrame()\n",
    "    logging.warning(\"No historical data available to merge.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------- API 4: Get a List of All Groups Endpoint --------\n",
    "group_list_url = 'https://api.example.com/v99/group_data'\n",
    "params_groups = {\n",
    "    'status': 'active',  # Returns a list of all active groups.\n",
    "    'per_page': 500  # Number of records per page (maximum allowed).\n",
    "}\n",
    "list_of_groups = fetch_data(group_list_url, headers, params_groups)\n",
    "\n",
    "# Process list of groups data\n",
    "if list_of_groups:\n",
    "    df_groups = pd.json_normalize(list_of_groups, sep='_')\n",
    "    \n",
    "    # Rename columns for appropriate ID references\n",
    "    df_groups.rename(columns={'id': 'group_id', 'name':'group_name'}, inplace=True)\n",
    "\n",
    "    logging.info(\"Group ID column renamed successfully.\")\n",
    "    logging.info(\"Groups DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 4.\")\n",
    "else:\n",
    "    blank_df_groups = pd.DataFrame()\n",
    "    logging.warning(\"No group data fetched.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------- API 5: Get a List of All Users in All Groups Endpoint --------\n",
    "# Ensure df_groups exists and contains the 'group_id' column before proceeding\n",
    "if 'df_groups' in locals() and 'group_id' in df_groups.columns:\n",
    "    group_ids = df_groups['group_id'].unique()  # Get unique group IDs\n",
    "else:\n",
    "    logging.error(\"DataFrame `df_groups` is either not defined or missing the 'group_id' column.\")\n",
    "    group_ids = []  # Set to empty to avoid further errors in looping\n",
    "\n",
    "# This will be done in a loop for each group\n",
    "params_group_users = {\n",
    "    'include_full_details': 'true',  # Include all user details\n",
    "    'per_page': 500  # Number of records per page (max 500)\n",
    "}\n",
    "group_ids = df_groups['group_id'].unique()  # Get unique group IDs\n",
    "\n",
    "all_users_all_groups_dfs = []\n",
    "for group_id in group_ids:\n",
    "    # Construct URL specific to each group\n",
    "    url_all_users_all_groups = f\"https://api.example.com/v99/groups/{group_id}/users\"\n",
    "    try:\n",
    "        # Make API call for each group\n",
    "        all_users_in_group = fetch_data(url_all_users_all_groups, headers, params_group_users)\n",
    "        \n",
    "        # Process the data if it's available\n",
    "        if all_users_in_group:\n",
    "            df = pd.DataFrame(all_users_in_group)\n",
    "                       \n",
    "            # Add 'group_id' column for group association\n",
    "            df['group_id'] = group_id\n",
    "            \n",
    "            # Append processed DataFrame to the list\n",
    "            all_users_all_groups_dfs.append(df)\n",
    "        else:\n",
    "            logging.warning(f\"No data fetched for group ID: {group_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for group ID {group_id}: {e}\")\n",
    "\n",
    "# Concatenate all user data across groups if any data was collected\n",
    "if all_users_all_groups_dfs:\n",
    "    combined_df_all_users_all_groups = pd.concat(all_users_all_groups_dfs, ignore_index=True)\n",
    "\n",
    "    # Convert dates to datetime where applicable\n",
    "    date_cols = ['joined_on', 'last_sign_in', 'employee_start_date', 'custom_date_1', 'custom_date_2']\n",
    "    convert_dates(combined_df_all_users_all_groups, date_cols)\n",
    "    # Rename ID columns for clarity\n",
    "    column_mappings = {'id': 'user_id', 'employee_number': 'internal_id'}\n",
    "    combined_df_all_users_all_groups = rename_and_cast_columns(combined_df_all_users_all_groups, column_mappings)\n",
    "\n",
    "    logging.info(\"Combined all users across groups DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 5.\")\n",
    "else:\n",
    "    blank_df_all_users_all_groups = pd.DataFrame()\n",
    "    logging.warning(\"No user data available to merge across groups.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------- API 6: Get a Specific Group's Historical Data Endpoint --------\n",
    "# This will also be done in a looped manner for each group similar to users\n",
    "group_ids = df_groups['group_id'].unique()\n",
    "params_group_history = {\n",
    "    'include_complete_history': 'true'  # Include the entire historical data of the group\n",
    "}\n",
    "\n",
    "# List to store individual group historical data\n",
    "group_history_dfs = []\n",
    "for group in group_ids:\n",
    "    # Construct URL specific to each group\n",
    "    url_group_history = f'https://api.example.com/v99/groups/{group}/history'\n",
    "    try:\n",
    "        # Make API call for each group\n",
    "        group_history = fetch_data(url_group_history, headers, params_group_history)\n",
    "        \n",
    "        # Process the data if it's available\n",
    "        if group_history:\n",
    "            df_group_history = pd.json_normalize(group_history, sep='_')\n",
    "            \n",
    "            # Convert dates to datetime if 'date' column exists\n",
    "            date_cols = ['date']\n",
    "            convert_dates(df_group_history, date_cols)\n",
    "            \n",
    "            # Add 'group_id' column to enable proper merging\n",
    "            df_group_history['group_id'] = group\n",
    "            \n",
    "            # Append processed DataFrame to the list\n",
    "            group_history_dfs.append(df_group_history)\n",
    "        else:\n",
    "            logging.warning(f\"No historical data fetched for group ID: {group}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for group ID {group}: {e}\")\n",
    "\n",
    "# Concatenate all group historical data DataFrames, if any\n",
    "if group_history_dfs:\n",
    "    combined_df_group_history = pd.concat(group_history_dfs, ignore_index=True)\n",
    "    \n",
    "    # Select only the necessary columns from df_groups for the merge\n",
    "    df_group_subset = df_groups[['group_id', 'group_name', 'member_count']]\n",
    "    \n",
    "    # Merge the combined history data with the subset of the original groups DataFrame\n",
    "    combined_df_group_history = pd.merge(combined_df_group_history, df_group_subset, \n",
    "                                         how='left', on='group_id')\n",
    "    logging.info(\"Combined Historical Data DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 6.\")\n",
    "else:\n",
    "    blank_df_combined_group_history = pd.DataFrame()\n",
    "    logging.warning(\"No historical data available to merge.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------- API 7: Get All Security Assessments Endpoint --------\n",
    "url_security_assessments = 'https://api.example.com/v99/security/assessments'\n",
    "params_security_assessments = {\n",
    "    'assessment_type': 'callback',  # Returns information related to callback type assessments.\n",
    "    'per_page': 500  # Number of records per page (maximum allowed).\n",
    "}\n",
    "\n",
    "data_security_assessments = fetch_data(url_security_assessments, headers, params_security_assessments)\n",
    "if data_security_assessments:\n",
    "    df_security_assessments = pd.json_normalize(data_security_assessments, sep='_')\n",
    "    \n",
    "    # Convert dates to datetime\n",
    "    date_cols = ['start_date', 'end_date']\n",
    "    convert_dates(df_security_assessments, date_cols)\n",
    "\n",
    "    logging.info(\"Security Assessments DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 7.\")\n",
    "else:\n",
    "    blank_df_security_assessments = pd.DataFrame()\n",
    "    logging.warning(\"No security assessment data fetched.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# -------- API 8: Get Specific Campaign Data Endpoint --------\n",
    "# Define unique campaign IDs from the security assessments dataframe\n",
    "campaign_ids = df_security_assessments['campaign_id'].unique()\n",
    "params_campaign = {\n",
    "    'per_page': 500  # Number of records per page (maximum allowed)\n",
    "}\n",
    "\n",
    "# Initialize lists to store campaign data and related group data\n",
    "campaign_data_dfs = []\n",
    "group_bridge_dfs = []\n",
    "\n",
    "for campaign in campaign_ids:\n",
    "    # Construct URL specific to each campaign\n",
    "    url_campaigns = f'https://api.example.com/v99/campaigns/{campaign}'\n",
    "    try:\n",
    "        # Make API call for each campaign\n",
    "        campaigns_data = fetch_data(url_campaigns, headers, params_campaign)\n",
    "        \n",
    "        # Process the data if available\n",
    "        if campaigns_data:\n",
    "            # Normalize the main campaign data, extracting nested records separately\n",
    "            df_campaign = pd.json_normalize(\n",
    "                campaigns_data, \n",
    "                record_path='details', \n",
    "                meta=['campaign_id', 'name', 'phish_prone_percentage', 'last_run', 'status', \n",
    "                      'send_duration', 'track_duration', 'frequency', 'create_date'],\n",
    "                record_prefix='detail_',\n",
    "                errors='ignore'\n",
    "            )\n",
    "            \n",
    "            df_groups_bridge = pd.json_normalize(\n",
    "                campaigns_data, \n",
    "                record_path='groups', \n",
    "                meta=['campaign_id', 'name'], \n",
    "                record_prefix='group_', \n",
    "                errors='ignore'\n",
    "            )\n",
    "            \n",
    "            # Add each normalized DataFrame to the respective lists\n",
    "            campaign_data_dfs.append(df_campaign)\n",
    "            group_bridge_dfs.append(df_groups_bridge)\n",
    "            \n",
    "            # Log success message\n",
    "            logging.info(f\"Data for Campaign ID {campaign} processed successfully.\")\n",
    "        else:\n",
    "            logging.warning(f\"No data fetched for Campaign ID: {campaign}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for Campaign ID {campaign}: {e}\")\n",
    "\n",
    "# Concatenate all campaign and group bridge dataframes if any data was collected\n",
    "if campaign_data_dfs:\n",
    "    combined_df_campaigns = pd.concat(campaign_data_dfs, ignore_index=True)\n",
    "    \n",
    "    # Define columns that need datetime conversion\n",
    "    date_columns = ['detail_start_date', 'last_run', 'create_date']\n",
    "    convert_dates(combined_df_campaigns, date_columns)\n",
    "\n",
    "    # Calculate 'campaign_end_date' by adding duration to 'last_run', if both columns exist\n",
    "    if 'last_run' in combined_df_campaigns.columns and 'duration_days' in combined_df_campaigns.columns:\n",
    "        combined_df_campaigns['campaign_end_date'] = combined_df_campaigns['last_run'] + pd.to_timedelta(combined_df_campaigns['duration_days'], unit='D')\n",
    "        convert_dates(combined_df_campaigns, ['campaign_end_date'])\n",
    "\n",
    "    logging.info(\"Combined Campaign Data DataFrame created successfully.\")\n",
    "else:\n",
    "    blank_df_campaigns = pd.DataFrame()\n",
    "    logging.warning(\"No campaign data available to merge.\")\n",
    "\n",
    "if group_bridge_dfs:\n",
    "    combined_df_groups_bridge = pd.concat(group_bridge_dfs, ignore_index=True)\n",
    "    \n",
    "    # Define column mappings for renaming\n",
    "    column_mappings = {\n",
    "        'group_group_id': 'group_id',\n",
    "    }\n",
    "    combined_df_groups_bridge = rename_and_cast_columns(combined_df_groups_bridge, column_mappings)\n",
    "    logging.info(\"Combined Groups-to-Campaign Bridge DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 8.\")\n",
    "else:\n",
    "    blank_df_groups_bridge = pd.DataFrame()\n",
    "    logging.warning(\"No group bridge data available to merge.\")\n",
    "\n",
    "\n",
    "# -------- API 9: Get All Assessment Results Endpoint --------\n",
    "# Looped approach for each security assessment\n",
    "assessment_ids = df_security_assessments['assessment_id'].unique()\n",
    "params_assessment_results = {\n",
    "    'per_page': 500  # Number of records per page (maximum allowed)\n",
    "}\n",
    "\n",
    "assessment_results_dfs = []  # Initialize list to store each assessment results DataFrame\n",
    "\n",
    "for assessment_id in assessment_ids:\n",
    "    # Construct URL specific to each assessment\n",
    "    url_assessment_results = f'https://api.example.com/v99/assessments/{assessment_id}/results'\n",
    "    try:\n",
    "        # Make API call for each assessment\n",
    "        assessment_data = fetch_data(url_assessment_results, headers, params_assessment_results)\n",
    "        \n",
    "        # Process the data if available\n",
    "        if assessment_data:\n",
    "            # Convert JSON data to a DataFrame\n",
    "            df = pd.DataFrame(assessment_data)\n",
    "\n",
    "            # Add a column for the Assessment ID to relate recipients back to the assessment\n",
    "            df['assessment_id'] = assessment_id\n",
    "\n",
    "            # Append the data to the results list\n",
    "            assessment_results_dfs.append(df)\n",
    "\n",
    "            logging.info(f\"Data for Assessment ID {assessment_id} processed successfully.\")\n",
    "        else:\n",
    "            logging.warning(f\"No data fetched for Assessment ID: {assessment_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for Assessment ID {assessment_id}: {e}\")\n",
    "\n",
    "# Concatenate all Assessment DataFrames if any data was collected\n",
    "if assessment_results_dfs:\n",
    "    combined_df_assessment_results = pd.concat(assessment_results_dfs, ignore_index=True)\n",
    "\n",
    "    # Normalize the DataFrame\n",
    "    combined_df_assessment_results = json_normalize(combined_df_assessment_results.to_dict(orient='records'), sep='_')\n",
    "    \n",
    "    # Date columns to convert, if any\n",
    "    date_columns = ['scheduled_at', 'delivered_at', 'opened_at',\n",
    "       'clicked_at', 'replied_at', 'attachment_opened_at', 'macro_enabled_at',\n",
    "       'data_entered_at', 'qr_code_scanned', 'reported_at', 'bounced_at']\n",
    "    convert_dates(combined_df_assessment_results, date_columns)\n",
    "\n",
    "    logging.info(\"Combined Assessment Results Data DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 9.\")\n",
    "else:\n",
    "    logging.warning(\"No Assessment data available to merge.\")\n",
    "    blank_assessment_results = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------- API 10: Get All Training Registrations Endpoint --------\n",
    "url_registrations = 'https://api.example.com/v99/training/registrations'\n",
    "params_registrations = {\n",
    "    'exclude_archived': 'false',  \n",
    "    'include_id_details': 'true',  \n",
    "    'include_purchase_info': 'true',\n",
    "    'per_page': 500,  # Number of records per page (maximum allowed)\n",
    "}\n",
    "\n",
    "data_registrations = fetch_data(url_registrations, headers, params_registrations)\n",
    "if data_registrations:\n",
    "    df_registrations = pd.json_normalize(data_registrations, sep='_')\n",
    "\n",
    "    # Convert dates to datetime\n",
    "    date_cols = ['enrollment_date', 'start_date', 'completion_date']\n",
    "    convert_dates(df_registrations, date_cols)\n",
    "\n",
    "    # Rename ID columns for clarity\n",
    "    column_mappings = {'user_id': 'registration_id'}\n",
    "    df_registrations = rename_and_cast_columns(df_registrations, column_mappings)\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_registrations['duration_hours'] = (df_registrations['completion_date'] - df_registrations['start_date']).dt.total_seconds() / 3600\n",
    "    df_registrations['duration_minutes'] = (df_registrations['completion_date'] - df_registrations['start_date']).dt.total_seconds() / 60\n",
    "    df_registrations['ongoing_flag'] = df_registrations['start_date'].notna() & df_registrations['completion_date'].isna()\n",
    "    df_registrations['completed_flag'] = df_registrations['completion_date'].notna().astype(int)\n",
    "    df_registrations['enrollment_to_start_days'] = (df_registrations['start_date'] - df_registrations['enrollment_date']).dt.days\n",
    "    df_registrations['start_to_completion_days'] = (df_registrations['completion_date'] - df_registrations['start_date']).dt.days\n",
    "    df_registrations['enrollment_to_completion_days'] = (df_registrations['completion_date'] - df_registrations['enrollment_date']).dt.days\n",
    "    current_date = pd.Timestamp.now().tz_localize(None)\n",
    "    df_registrations['enrollment_duration_days'] = (current_date - df_registrations['enrollment_date']).dt.days\n",
    "\n",
    "    logging.info(\"Training Registrations DataFrame created successfully.\")\n",
    "    logging.info(\"Completed call: API 10.\")\n",
    "else:\n",
    "    blank_df_registrations = pd.DataFrame()\n",
    "    logging.warning(\"No registration data fetched.\")\n",
    "\n",
    "\n",
    "# -------- API 11: Get All Educational Programs Endpoint --------\n",
    "url_programs = 'https://api.example.com/v99/educational/programs'\n",
    "params_programs = {\n",
    "    'include_details': 'true',\n",
    "    'per_page': 500  # Number of records per page (maximum allowed)\n",
    "}\n",
    "\n",
    "data_programs = fetch_data(url_programs, headers, params_programs)\n",
    "\n",
    "if data_programs:\n",
    "    # Initial flattening of the programs data\n",
    "    df_programs = pd.json_normalize(data_programs, sep='_')\n",
    "    \n",
    "    # Flatten group and content fields (before applying datetime conversion)\n",
    "    df_program_details = pd.json_normalize(\n",
    "        data_programs,\n",
    "        record_path=['groups'],\n",
    "        meta=['program_id', 'name', 'status', 'duration_type', 'start_date', 'end_date', 'auto_enroll', 'completion_rate'],\n",
    "        record_prefix='group_',\n",
    "        errors='ignore'\n",
    "    )\n",
    "    \n",
    "    df_program_content = pd.json_normalize(\n",
    "        data_programs,\n",
    "        record_path=['content'],\n",
    "        meta=['program_id', 'name', 'status', 'duration_type', 'start_date', 'end_date', 'auto_enroll', 'completion_rate'],\n",
    "        record_prefix='content_',\n",
    "        errors='ignore'\n",
    "    )\n",
    "\n",
    "    # Convert dates to datetime after all json_normalize operations\n",
    "    date_cols = ['start_date', 'end_date']\n",
    "    for c in date_cols:\n",
    "        if c in df_programs.columns:\n",
    "            convert_dates(df_programs, date_cols)\n",
    "        if c in df_program_details.columns:\n",
    "            convert_dates(df_program_details, date_cols)\n",
    "        if c in df_program_content.columns:\n",
    "            convert_dates(df_program_content, date_cols)\n",
    "\n",
    "        # Rename the group_id column for clarity\n",
    "        df_program_details.rename(columns={'group_group_id': 'group_id'}, inplace=True)\n",
    "\n",
    "    logging.info(\"Educational Programs DataFrames created successfully.\")\n",
    "    logging.info(\"Completed call: API 11.\")\n",
    "else:\n",
    "    blank_df_programs = pd.DataFrame()\n",
    "    blank_df_program_details = pd.DataFrame()\n",
    "    blank_df_program_content = pd.DataFrame()\n",
    "    logging.warning(\"No educational program data fetched.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Final DataFrames for Power BI; Merging and Conversion\n",
    "\n",
    "# For df_registrations, merge with df_programs to include program id, start, and end dates\n",
    "# Merge by first selecting the columns to comprise the right-hand dataset\n",
    "df_right = df_programs[['program_id', 'start_date', 'end_date']]\n",
    "df_registrations = pd.merge(df_registrations, df_right, how='left', on='program_id')\n",
    "\n",
    "# Rename the newly merged columns to avoid conflicts\n",
    "df_registrations.rename(columns={'start_date_x': 'start_date',\n",
    "                                 'start_date_y': 'program_start_date',\n",
    "                                 'end_date': 'program_end_date'},\n",
    "                        inplace=True)\n",
    "\n",
    "logging.info(\"Merged Registrations with Programs DataFrame created successfully.\")\n",
    "\n",
    "# Perform the merge to include 'program_id' in combined_df_assessment_results\n",
    "combined_df_assessment_results_merged = combined_df_assessment_results.merge(\n",
    "    df_security_assessments[['assessment_id', 'program_id']], \n",
    "    on='assessment_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "logging.info(\"Merged Assessment Results with Programs DataFrame created successfully.\")\n",
    "\n",
    "# Log the start of the script that converts ID columns to int64\n",
    "logging.info(\"Starting conversion of ID columns to int64.\")\n",
    "\n",
    "# Dictionary of your DataFrames; the keys are the table names that will be inserted into database, and the values are the DataFrames generated from the API calls\n",
    "dataframes = {\n",
    "    'scia_api_educational_account_history': df_group_history,\n",
    "    'scia_api_educational_users': df_user_list,\n",
    "    'scia_api_educational_user_history': combined_df_group_history,\n",
    "    'scia_api_educational_groups': df_groups,\n",
    "    'scia_api_educational_users_in_groups': combined_df_all_users_all_groups,\n",
    "    'scia_api_educational_group_history': combined_df_group_history,\n",
    "    'scia_api_educational_assessments': df_security_assessments,\n",
    "    'scia_api_educational_programs': df_programs,\n",
    "    'scia_api_educational_program_details': df_program_details,\n",
    "    'scia_api_educational_content': df_program_content,\n",
    "    'scia_api_educational_registrations': df_registrations,\n",
    "    'scia_api_educational_assessment_results': combined_df_assessment_results,\n",
    "    'scia_api_educational_groups_bridge': combined_df_groups_bridge,\n",
    "    'scia_api_educational_assessment_results_merged': combined_df_assessment_results_merged\n",
    "}\n",
    "\n",
    "id_columns = [\n",
    "    'program_id', \n",
    "    'content_purchase_id', \n",
    "    'registration_id', \n",
    "    'group_id', \n",
    "    'internal_id',\n",
    "    'user_id', \n",
    "    'detail_id', \n",
    "    'recipient_id'\n",
    "]\n",
    "\n",
    "# Apply the helper function to the dataframes dictionary\n",
    "convert_id_columns_to_int64(dataframes, id_columns)\n",
    "\n",
    "# Assertion to check the ID conversion with an early exit if any conversion fails\n",
    "for table_name, df in dataframes.items():\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                assert df[col].dtype == 'int64', f\"Column '{col}' in DataFrame '{table_name}' is not of type int64.\"\n",
    "            except AssertionError as e:\n",
    "                logging.error(e)\n",
    "                exit()\n",
    "                \n",
    "logging.info(\"Completed conversion of ID columns to int64.\")\n",
    "# Log the end of the script and signal that we are moving to the next step\n",
    "logging.info(f\"API script completed successfully; returned back are {len(dataframes)} dataframes processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
