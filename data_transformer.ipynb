{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Project Overview: Automated Pipeline for API Integration\n",
    "# -------------------------------------------------------------------------------\n",
    "# This project automates the process of calling an API, storing data in a \n",
    "# structured SQL database, and preparing it for analysis in Power BI.\n",
    "# \n",
    "# Below is an outline of the three main stages:\n",
    "#\n",
    "# 1. Pipeline: API Call\n",
    "#    - Purpose: Retrieve data from the API, including information on users, \n",
    "#      groups, risk scores, phishing tests, and training campaigns.\n",
    "#    - Description: This stage uses API endpoints to gather raw data, transforming it \n",
    "#      into a structured format for ingestion.\n",
    "#\n",
    "# 2. Pipeline: SQL Table and Schema Creation\n",
    "#    - Purpose: Set up the SQL database structure for storing data.\n",
    "#    - Description: This stage defines tables and schemas for all required entities, \n",
    "#      including users, groups, risk score histories, and phishing and training details.\n",
    "#      It includes drop-and-create commands to ensure the tables are up-to-date with the \n",
    "#      most recent schema and enforce data integrity.\n",
    "#\n",
    "# 3. Pipeline: SQL Table Insertion\n",
    "#    - Purpose: Insert the retrieved and processed data from the API into the SQL tables.\n",
    "#    - Description: This stage handles the actual insertion of data into the SQL database \n",
    "#      while preserving foreign key constraints and ensuring that data types are aligned \n",
    "#      with the database schema. All data insertions are done within a transaction for \n",
    "#      atomicity, meaning that any insertion failure results in a full rollback, preserving \n",
    "#      database consistency.\n",
    "#\n",
    "# -------------------------------------------------------------------------------\n",
    "# End of Project Overview\n",
    "# -------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# SQL Table Setup for ETL Pipeline\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# This script is responsible for the setup and teardown of SQL tables to store data fetched from various APIs in the Extract phase of our ETL pipeline.\n",
    "# It is designed to manage database schemas with careful consideration of dependencies, ensuring that all operations adhere to best practices for data integrity and referential integrity.\n",
    "\n",
    "# We utilize two dictionaries to manage SQL commands:\n",
    "# 1. `create_table_sql_commands`: Stores SQL statements for creating tables, ensuring they are executed in the correct dependency order.\n",
    "# 2. `drop_table_sql_commands`: Stores SQL statements for dropping tables, executed in reverse dependency order to handle foreign key constraints without errors.\n",
    "\n",
    "# Splitting the commands into separate dictionaries allows for efficient and safe rebuilding of the database schema during development and operational phases.\n",
    "create_table_sql_commands = {\n",
    "\n",
    "    \"table_1\": \"\"\"\n",
    "        CREATE TABLE dbo.TABLE_1 (\n",
    "            COLUMN_1 DECIMAL(10, 4) NOT NULL CHECK (COLUMN_1 >= 0),\n",
    "            COLUMN_2 DATETIME PRIMARY KEY\n",
    "        );\n",
    "    \"\"\",\n",
    "\n",
    "    \"table_2\": \"\"\"\n",
    "        CREATE TABLE dbo.TABLE_2 (\n",
    "            COLUMN_1 INT UNIQUE,\n",
    "            COLUMN_2 INT PRIMARY KEY NOT NULL,\n",
    "            COLUMN_3 NVARCHAR(50) NULL,\n",
    "            COLUMN_4 NVARCHAR(50) NULL,\n",
    "            COLUMN_5 NVARCHAR(100) NULL,\n",
    "            COLUMN_6 NVARCHAR(255) UNIQUE NULL,\n",
    "            COLUMN_7 NVARCHAR(20) NULL,\n",
    "            COLUMN_8 NVARCHAR(10) NULL,\n",
    "            COLUMN_9 NVARCHAR(20) NULL,\n",
    "            COLUMN_10 NVARCHAR(100) NULL,\n",
    "            COLUMN_11 NVARCHAR(100) NULL,\n",
    "            COLUMN_12 NVARCHAR(100) NULL,\n",
    "            COLUMN_13 NVARCHAR(100) NULL,\n",
    "            COLUMN_14 NVARCHAR(50) NULL,\n",
    "            COLUMN_15 NVARCHAR(100) NULL,\n",
    "            COLUMN_16 NVARCHAR(255) NULL,\n",
    "            COLUMN_17 NVARCHAR(50) NULL,\n",
    "            COLUMN_18 TEXT NULL,\n",
    "            COLUMN_19 BIT NULL DEFAULT 0,\n",
    "            COLUMN_20 NVARCHAR(255) NULL,\n",
    "            COLUMN_21 DATETIME NULL,\n",
    "            COLUMN_22 DATETIME NULL,\n",
    "            COLUMN_23 DATETIME NULL,\n",
    "            COLUMN_24 DATETIME NULL,\n",
    "            COLUMN_25 DATETIME NULL,\n",
    "            COLUMN_26 DECIMAL(10, 4) NULL CHECK (COLUMN_26 >= 0 AND COLUMN_26 <= 100),\n",
    "            COLUMN_27 DECIMAL(10, 4) NULL CHECK (COLUMN_27 >= 0),\n",
    "            COLUMN_28 NVARCHAR(MAX) NULL,\n",
    "            COLUMN_29 NVARCHAR(MAX) NULL,\n",
    "            COLUMN_30 TEXT NULL,\n",
    "            COLUMN_31 TEXT NULL,\n",
    "            COLUMN_32 TEXT NULL,\n",
    "            COLUMN_33 TEXT NULL\n",
    "        );\n",
    "    \"\"\",\n",
    "\n",
    "    \"table_3\": \"\"\"\n",
    "        CREATE TABLE dbo.TABLE_3 (\n",
    "            COLUMN_1 INT NOT NULL,\n",
    "            COLUMN_2 INT NOT NULL,\n",
    "            COLUMN_3 DATETIME NOT NULL,\n",
    "            COLUMN_4 FLOAT NOT NULL CHECK (COLUMN_4 >= 0),\n",
    "            COLUMN_5 NVARCHAR(50) NULL,\n",
    "            COLUMN_6 NVARCHAR(50) NULL,\n",
    "            COLUMN_7 NVARCHAR(100) NULL,\n",
    "            COLUMN_8 NVARCHAR(100) NULL,\n",
    "            COLUMN_9 NVARCHAR(100) NULL,\n",
    "            CONSTRAINT PK_TABLE_3 PRIMARY KEY (COLUMN_2, COLUMN_3),\n",
    "            CONSTRAINT CHK_COLUMN_4_POSITIVE CHECK (COLUMN_4 >= 0)\n",
    "        );\n",
    "    \"\"\",\n",
    "\n",
    "    \"table_4\": \"\"\"\n",
    "        CREATE TABLE dbo.TABLE_4 (\n",
    "            COLUMN_1 INT PRIMARY KEY NOT NULL,\n",
    "            COLUMN_2 NVARCHAR(255) NULL,\n",
    "            COLUMN_3 NVARCHAR(100) NULL,\n",
    "            COLUMN_4 NVARCHAR(50) NULL,\n",
    "            COLUMN_5 NVARCHAR(50) NULL,\n",
    "            COLUMN_6 INT NOT NULL DEFAULT 0 CHECK (COLUMN_6 >= 0),\n",
    "            COLUMN_7 DECIMAL(10, 1) NULL CHECK (COLUMN_7 >= 0)\n",
    "        );\n",
    "    \"\"\",\n",
    "\n",
    "    \"table_5\": \"\"\"\n",
    "        CREATE TABLE dbo.TABLE_5 (\n",
    "            COLUMN_1 INT NOT NULL,\n",
    "            COLUMN_2 INT NULL,\n",
    "            COLUMN_3 INT NOT NULL,\n",
    "            COLUMN_4 NVARCHAR(50) NULL,\n",
    "            COLUMN_5 NVARCHAR(50) NULL,\n",
    "            COLUMN_6 NVARCHAR(100) NULL,\n",
    "            COLUMN_7 NVARCHAR(255) NULL,\n",
    "            COLUMN_8 NVARCHAR(20) NULL,\n",
    "            COLUMN_9 NVARCHAR(10) NULL,\n",
    "            COLUMN_10 NVARCHAR(20) NULL,\n",
    "            COLUMN_11 NVARCHAR(100) NULL,\n",
    "            COLUMN_12 NVARCHAR(100) NULL,\n",
    "            COLUMN_13 NVARCHAR(100) NULL,\n",
    "            COLUMN_14 NVARCHAR(100) NULL,\n",
    "            COLUMN_15 NVARCHAR(50) NULL,\n",
    "            COLUMN_16 NVARCHAR(100) NULL,\n",
    "            COLUMN_17 NVARCHAR(255) NULL,\n",
    "            COLUMN_18 NVARCHAR(50) NULL,\n",
    "            COLUMN_19 TEXT NULL,\n",
    "            COLUMN_20 FLOAT NULL CHECK (COLUMN_20 >= 0 AND COLUMN_20 <= 100),\n",
    "            COLUMN_21 FLOAT NULL CHECK (COLUMN_21 >= 0),\n",
    "            COLUMN_22 BIT NULL DEFAULT 0,\n",
    "            COLUMN_23 NVARCHAR(255) NULL,\n",
    "            COLUMN_24 TEXT NULL,\n",
    "            COLUMN_25 TEXT NULL,\n",
    "            COLUMN_26 DATETIME NULL,\n",
    "            COLUMN_27 DATETIME NULL,\n",
    "            COLUMN_28 DATETIME NULL,\n",
    "            COLUMN_29 DATETIME NULL,\n",
    "            COLUMN_30 TEXT NULL,\n",
    "            COLUMN_31 TEXT NULL,\n",
    "            COLUMN_32 TEXT NULL,\n",
    "            COLUMN_33 TEXT NULL,\n",
    "            CONSTRAINT PK_TABLE_5 PRIMARY KEY (COLUMN_1, COLUMN_3),\n",
    "            CONSTRAINT CHK_COLUMN_20 CHECK (COLUMN_20 >= 0 AND COLUMN_20 <= 100),\n",
    "            CONSTRAINT CHK_COLUMN_21 CHECK (COLUMN_21 >= 0)\n",
    "        );\n",
    "    \"\"\",\n",
    "\n",
    "    \"table_6\": \"\"\"\n",
    "        CREATE TABLE dbo.TABLE_6 (\n",
    "            COLUMN_1 INT NOT NULL,\n",
    "            COLUMN_2 DATETIME NOT NULL,\n",
    "            COLUMN_3 FLOAT NOT NULL CHECK (COLUMN_3 >= 0),\n",
    "            COLUMN_4 NVARCHAR(255) NULL,\n",
    "            COLUMN_5 INT NOT NULL,\n",
    "            CONSTRAINT PK_TABLE_6 PRIMARY KEY (COLUMN_1, COLUMN_2)\n",
    "        );\n",
    "    \"\"\",\n",
    "\n",
    "    \"table_7\": \"\"\"\n",
    "        CREATE TABLE dbo.TABLE_7 (\n",
    "            COLUMN_1 BIGINT NOT NULL,\n",
    "            COLUMN_2 BIGINT NOT NULL,\n",
    "            COLUMN_3 NVARCHAR(50) NULL,\n",
    "            COLUMN_4 NVARCHAR(255) NULL,\n",
    "            COLUMN_5 INT CHECK (COLUMN_5 >= 0) NULL,\n",
    "            COLUMN_6 FLOAT CHECK (COLUMN_6 >= 0 AND COLUMN_6 <= 100) NULL,\n",
    "            COLUMN_7 INT CHECK (COLUMN_7 >= 0) NULL,\n",
    "            COLUMN_8 INT CHECK (COLUMN_8 >= 0) NULL,\n",
    "            COLUMN_9 INT CHECK (COLUMN_9 >= 0) NULL,\n",
    "            COLUMN_10 INT CHECK (COLUMN_10 >= 0) NULL,\n",
    "            COLUMN_11 INT CHECK (COLUMN_11 >= 0) NULL,\n",
    "            COLUMN_12 INT CHECK (COLUMN_12 >= 0) NULL,\n",
    "            COLUMN_13 DATETIME NULL,\n",
    "            COLUMN_14 TEXT NULL,\n",
    "            CONSTRAINT PK_TABLE_7 PRIMARY KEY (COLUMN_1, COLUMN_2),\n",
    "            CONSTRAINT CHK_COLUMN_6 CHECK (COLUMN_6 >= 0 AND COLUMN_6 <= 100),\n",
    "            CONSTRAINT CHK_COLUMN_5 CHECK (COLUMN_5 >= 0)\n",
    "        );\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "drop_table_sql_commands = {\n",
    "    \"table_1\": \"DROP TABLE IF EXISTS dbo.TABLE_1;\",\n",
    "    \"table_2\": \"DROP TABLE IF EXISTS dbo.TABLE_2;\",\n",
    "    \"table_3\": \"DROP TABLE IF EXISTS dbo.TABLE_3;\",\n",
    "    \"table_4\": \"DROP TABLE IF EXISTS dbo.TABLE_4;\",\n",
    "    \"table_5\": \"DROP TABLE IF EXISTS dbo.TABLE_5;\",\n",
    "    \"table_6\": \"DROP TABLE IF EXISTS dbo.TABLE_6;\",\n",
    "    \"table_7\": \"DROP TABLE IF EXISTS dbo.TABLE_7;\"\n",
    "}\n",
    "\n",
    "# Return the dictionary for review\n",
    "logging.info(f\"Returned back are {len(create_table_sql_commands)} SQL commands for table creation.\")\n",
    "logging.info(f\"Returned back are {len(drop_table_sql_commands)} SQL commands for table deletion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Database Schema Management Script\n",
    "# -------------------------------------------------------------------------------\n",
    "# This script is responsible for managing the schema for the ETL pipeline, ensuring \n",
    "# all tables are created or dropped in a consistent and safe manner. The approach \n",
    "# ensures:\n",
    "# 1. All-or-Nothing Execution: If any operation fails, all changes are rolled back.\n",
    "# 2. Referential Integrity: Tables are dropped and recreated in the correct order \n",
    "#    based on their dependencies.\n",
    "# 3. Detailed Logging: Each operation is logged to provide transparency and aid debugging.\n",
    "# 4. Verification: Post-creation checks ensure all tables exist as expected.\n",
    "\n",
    "# Key Steps:\n",
    "# 1. Load environment variables and establish a database connection.\n",
    "# 2. Drop existing tables in reverse dependency order to handle foreign key constraints.\n",
    "# 3. Create new tables in the correct dependency order using SQL commands stored in dictionaries.\n",
    "# 4. Verify the existence of all tables after the transaction commits.\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"api_script.log\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load connection string from environment\n",
    "connection_string = os.getenv(\"CONNECT_STR\")\n",
    "if not connection_string:\n",
    "    logging.error(\"Database connection string not found in environment variables.\")\n",
    "    raise ValueError(\"Database connection string is required.\")\n",
    "\n",
    "# Start the logging process with a timestamp\n",
    "start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "logging.info(f\"Starting table creation process at {start_time}.\")\n",
    "\n",
    "try:\n",
    "    # Create the database engine\n",
    "    engine = create_engine(connection_string)\n",
    "    logging.info(\"Database engine created successfully.\")\n",
    "\n",
    "    # Test the connection and retrieve the database version\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT @@VERSION;\"))\n",
    "        db_version = result.fetchone()[0]\n",
    "        logging.info(f\"Connected to database, version: {db_version}\")\n",
    "\n",
    "    # Begin a transaction to drop and recreate tables\n",
    "    with engine.begin() as conn:\n",
    "        try:\n",
    "            # Drop tables in reverse dependency order\n",
    "            logging.info(\"Starting table drop process.\")\n",
    "            for table_name, drop_table_sql in reversed(drop_table_sql_commands.items()):\n",
    "                # Check if the table exists before attempting to drop it\n",
    "                result = conn.execute(text(\n",
    "                    f\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_name.upper()}';\"\n",
    "                ))\n",
    "                table_exists = result.fetchone()\n",
    "                \n",
    "                if table_exists:\n",
    "                    conn.execute(text(drop_table_sql))\n",
    "                    logging.info(f\"Table '{table_name}' dropped successfully.\")\n",
    "                else:\n",
    "                    logging.info(f\"Table '{table_name}' does not exist, skipping drop.\")\n",
    "\n",
    "            # Create tables in the correct dependency order\n",
    "            logging.info(\"Starting table creation process.\")\n",
    "            for table_name, create_table_sql in create_table_sql_commands.items():\n",
    "                conn.execute(text(create_table_sql))\n",
    "                logging.info(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "            # Commit the transaction only after all tables are dropped and recreated\n",
    "            logging.info(\"All tables created successfully in a single transaction.\")\n",
    "\n",
    "        except SQLAlchemyError as e:\n",
    "            # If any command fails, rollback the entire transaction\n",
    "            logging.error(f\"Operation failed, rolling back transaction: {e}\")\n",
    "            raise\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    logging.error(f\"An error occurred in the table creation process: {e}\")\n",
    "\n",
    "\n",
    "# Confirm the existence of each table if the transaction committed successfully\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        for table_name in create_table_sql_commands.keys():\n",
    "            try:\n",
    "                result = conn.execute(text(\n",
    "                    f\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_name.upper()}';\"\n",
    "                ))\n",
    "                table_exists = result.fetchone()\n",
    "                if table_exists:\n",
    "                    logging.info(f\"Table '{table_name}' exists in the database.\")\n",
    "                else:\n",
    "                    logging.warning(f\"Table '{table_name}' not found in the database.\")\n",
    "            except SQLAlchemyError as e:\n",
    "                logging.error(f\"Error while checking existence of table '{table_name}': {e}\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    logging.error(f\"Error in final table existence verification process: {e}\")\n",
    "\n",
    "# Final log entry to mark end of the script\n",
    "end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "logging.info(f\"Table creation and verification process completed at {end_time}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Insertion Order Configuration\n",
    "# -------------------------------------------------------------------------------\n",
    "# This script manages data insertion for tables with complex foreign key dependencies.\n",
    "# To ensure smooth data insertion without violating foreign key constraints, we define\n",
    "# an `insertion_order` list.\n",
    "\n",
    "# `insertion_order`:\n",
    "#    - Specifies the sequence for inserting data into tables based on dependencies.\n",
    "#    - By inserting data into tables that do not depend on others first, followed by\n",
    "#      tables that depend on data in previous tables, we prevent foreign key constraint\n",
    "#      violations.\n",
    "#    - This ordered approach preserves referential integrity across the database and \n",
    "#      ensures that each table’s dependencies are satisfied.\n",
    "\n",
    "# Using only `insertion_order`, we set up a reliable workflow for data insertion\n",
    "# that upholds data integrity without requiring truncation, as tables are dropped and \n",
    "# recreated prior to this step.\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# Insertion order - correct dependency order for inserting data\n",
    "insertion_order = [\n",
    "    \"TABLE_1\",   \n",
    "    \"TABLE_2\",                \n",
    "    \"TABLE_3\",     \n",
    "    \"TABLE_4\",                       \n",
    "    \"TABLE_5\",              \n",
    "    \"TABLE_6\",    \n",
    "    \"TABLE_7\"   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Configure logging with DEBUG level for more detailed output during development\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Change to INFO or WARNING in production\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"api_script.log\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def truncate_large_int(value, length=8):\n",
    "    \"\"\"\n",
    "    Truncates an integer to the specified number of rightmost digits.\n",
    "    \n",
    "    Parameters:\n",
    "        value (int): The integer to be truncated.\n",
    "        length (int): The number of rightmost digits to keep (default is 8).\n",
    "    \n",
    "    Returns:\n",
    "        int: The truncated integer.\n",
    "    \"\"\"\n",
    "    # Ensure value is an integer and truncate if it's large\n",
    "    if isinstance(value, int) and value > 10**length:\n",
    "        return int(str(value)[-length:])\n",
    "    return value\n",
    "\n",
    "def prepare_for_insertion(df):\n",
    "    \"\"\"\n",
    "    Pre-processes a DataFrame for SQL insertion by setting the 'groups', 'aliases', and 'content' columns to None if they exist.\n",
    "    These columns often contain JSON-like structures that can be safely nulled because they are represented in bridge tables in our schema.\n",
    "    Additionally, this function formats datetime columns, rounds float values, and replaces NaNs in float columns.\n",
    "    \"\"\"\n",
    "    # Set 'groups' column to None if it exists\n",
    "    if 'groups' in df.columns:\n",
    "        df['groups'] = None\n",
    "        # found in dataframes: scia_api_users_listing, scia_api_users_in_groups, scia_api_training_campaigns\n",
    "    \n",
    "    if 'aliases' in df.columns:\n",
    "        df['aliases'] = None\n",
    "        # found in dataframes: scia_api_users_listing, scia_api_users_in_groups\n",
    "\n",
    "    if 'content' in df.columns:\n",
    "        df['content'] = None\n",
    "        # found in dataframes: scia_api_training_campaigns, scia_api_training_content\n",
    "    \n",
    "    # Format datetime columns to SQL-compatible strings\n",
    "    datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "    df[datetime_columns] = df[datetime_columns].map(lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(x) else None)\n",
    "    \n",
    "    # Round float columns and replace NaNs with a default value\n",
    "    float_columns = df.select_dtypes(include=['float']).columns\n",
    "    df[float_columns] = df[float_columns].fillna(0.0).round(4)\n",
    "    \n",
    "    # Truncate large integers in specific columns\n",
    "    if 'recipient_id' in df.columns:\n",
    "        df['recipient_id'] = df['recipient_id'].apply(lambda x: truncate_large_int(x, length=8))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_test_user(df, test_id=0000000):\n",
    "    \"\"\"\n",
    "    Removes rows where '_id' matches the given test_id.\n",
    "    \"\"\"\n",
    "    return df[df['_id'] != test_id].copy()\n",
    "\n",
    "def generate_insert_sql(table_name, df):\n",
    "    \"\"\"\n",
    "    Generates a parameterized SQL INSERT statement for the given table and DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        table_name (str): The name of the target SQL table.\n",
    "        df (pd.DataFrame): DataFrame containing the columns for the table.\n",
    "\n",
    "    Returns:\n",
    "        str: A SQL INSERT statement with parameter placeholders.\n",
    "    \"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    column_names = \", \".join(columns)\n",
    "    placeholders = \", \".join([f\":{col}\" for col in columns])\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO {table_name} ({column_names})\n",
    "    VALUES ({placeholders})\n",
    "    \"\"\"\n",
    "    return sql\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary of tables with dynamically generated SQL statements\n",
    "insertion_data = {\n",
    "    \"scia_api_account_risk_score_history\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_account_risk_score_history),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_account_risk_score_history\", api_df_account_risk_score_history)\n",
    "    },\n",
    "    \"scia_api_users_listing\": {\n",
    "        \"dataframe\": remove_test_user(prepare_for_insertion(api_df_users_listing)),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_users_listing\", api_df_users_listing)\n",
    "    },\n",
    "    \"scia_api_users_risk_score_history\": {\n",
    "        \"dataframe\": remove_test_user(prepare_for_insertion(api_df_combined_risk_score_history)),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_users_risk_score_history\", api_df_combined_risk_score_history)\n",
    "    },\n",
    "    \"scia_api_groups\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_groups),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_groups\", api_df_groups)\n",
    "    },\n",
    "    \"scia_api_users_in_groups\": {\n",
    "        \"dataframe\": remove_test_user(prepare_for_insertion(api_df_all_users_all_groups)),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_users_in_groups\", api_df_all_users_all_groups)\n",
    "    },\n",
    "    \"scia_api_group_risk_score_history\" : {\n",
    "        \"dataframe\" : prepare_for_insertion(api_df_combined_group_risk_score_history),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_group_risk_score_history\", api_df_combined_group_risk_score_history)\n",
    "    },\n",
    "    \"scia_api_phishing_tests\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_phishing_tests),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_phishing_tests\", api_df_phishing_tests)\n",
    "    },\n",
    "    \"scia_api_phishing_campaigns\": {\n",
    "        \"dataframe\" : prepare_for_insertion(api_df_combined_phish_campaigns),\n",
    "        \"insert_sql\" : generate_insert_sql(\"dbo.scia_api_phishing_campaigns\", api_df_combined_phish_campaigns)\n",
    "    }, \n",
    "    \"scia_api_pst_results\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_pst_results_merged),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_pst_results\", api_df_pst_results_merged)\n",
    "    },\n",
    "    \"scia_api_training_campaigns\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_campaigns),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_training_campaigns\", api_df_campaigns)\n",
    "    },\n",
    "    \"scia_api_training_enrollments\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_enrollments),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_training_enrollments\", api_df_enrollments)\n",
    "    }, \n",
    "    \"scia_api_training_details\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_training_details),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_training_details\", api_df_training_details)\n",
    "    },\n",
    "    \"scia_api_training_content\": {\n",
    "        \"dataframe\": prepare_for_insertion(api_df_content),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_training_content\", api_df_content)\n",
    "    },\n",
    "    \"scia_api_groups_bridge_campaigns\":{\n",
    "        \"dataframe\": prepare_for_insertion(api_df_combined_groups_bridge_campaigns),\n",
    "        \"insert_sql\": generate_insert_sql(\"dbo.scia_api_groups_bridge_campaigns\", api_df_combined_groups_bridge_campaigns)\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Logging output to verify generated SQL commands for each table\n",
    "for table_name, data in insertion_data.items():\n",
    "    logging.info(f\"INSERT SQL for table '{table_name}':\\n{data['insert_sql']}\\n\")\n",
    "\n",
    "logging.info(f\"{len(insertion_data)} SQL Insert statements generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
