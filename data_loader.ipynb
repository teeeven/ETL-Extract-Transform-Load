{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Data Loader for ETL Pipeline\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# This script is responsible for loading transformed data into SQL tables in the Load phase of our ETL pipeline.\n",
    "# It handles the bulk insertion of data while ensuring data integrity through careful transaction management and dependency handling.\n",
    "\n",
    "# Key Features:\n",
    "# 1. **Bulk Insertion**:\n",
    "#    - Uses the `bulk_insert` function to insert DataFrame rows into SQL tables efficiently.\n",
    "#    - Dynamically generates SQL `INSERT` statements based on DataFrame columns.\n",
    "#\n",
    "# 2. **Transaction Management**:\n",
    "#    - Wraps all insertions in a single transaction, ensuring atomicity (all-or-nothing behavior).\n",
    "#    - Rolls back changes if any insertion fails, preserving database integrity.\n",
    "#\n",
    "# 3. **Dependency-Aware Insertion**:\n",
    "#    - Inserts data in the correct order using the `insertion_order` list to respect foreign key relationships.\n",
    "#\n",
    "# 4. **Logging**:\n",
    "#    - Provides detailed logs of the insertion process, including start and end times, per-table progress, and error reporting.\n",
    "#\n",
    "# 5. **Scalable Design**:\n",
    "#    - Can handle large datasets using efficient bulk operations and supports a variety of table schemas.\n",
    "\n",
    "# Notes:\n",
    "# - The script assumes that the `insertion_data` dictionary contains pre-validated DataFrames for each table.\n",
    "# - Requires a valid `.env` file with the database connection string defined as `CONNECT_STR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError, IntegrityError\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"api_script.log\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load connection string from environment\n",
    "connection_string = os.getenv(\"CONNECT_STR\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the bulk insert function\n",
    "def bulk_insert(table_name, df, connection):\n",
    "    \"\"\"\n",
    "    Inserts multiple rows from a DataFrame into a specified SQL table in a single bulk operation.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    table_name : str\n",
    "        The name of the SQL table to insert data into.\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the data to be inserted. Each column in the DataFrame must match a column in the SQL table.\n",
    "    connection : sqlalchemy.engine.Connection\n",
    "        The active database connection for executing the bulk insert.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    SQLAlchemyError\n",
    "        Raised if an error occurs during the SQL execution. This will trigger a rollback in the main transaction block.\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    This function performs a bulk insert by generating a parameterized SQL `INSERT` statement with placeholders for each\n",
    "    column in the DataFrame. Using `executemany` under the hood, SQLAlchemy efficiently inserts all rows in one operation,\n",
    "    significantly improving performance over row-by-row insertion instead of previously defined insert_data function that does a row-by-row insertion.\n",
    "    \"\"\"\n",
    "    columns = \", \".join(df.columns)\n",
    "    placeholders = \", \".join([f\":{col}\" for col in df.columns])\n",
    "    insert_sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "    data_to_insert = df.to_dict(orient=\"records\")\n",
    "\n",
    "    try:\n",
    "        # Execute bulk insert\n",
    "        connection.execute(text(insert_sql), data_to_insert)\n",
    "        logging.info(f\"Inserted {len(data_to_insert)} rows into {table_name}.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"Error during bulk insert for {table_name}: {e}\")\n",
    "        raise  # Re-raise the exception for main transaction handling\n",
    "\n",
    "# Start logging the data insertion process\n",
    "start_time = datetime.now()\n",
    "logging.info(f\"Starting data insertion process at {start_time}.\")\n",
    "\n",
    "# Establish database connection and begin transaction for all insertions\n",
    "engine = create_engine(connection_string)\n",
    "with engine.connect() as conn:\n",
    "    transaction = conn.begin()\n",
    "    try:\n",
    "        # Insert data in the specified order\n",
    "        for table_name in insertion_order:\n",
    "            if table_name in insertion_data:\n",
    "                logging.info(f\"Starting insertion for table '{table_name}'.\")\n",
    "                start = time.time()\n",
    "                data = insertion_data[table_name]\n",
    "                df = data[\"dataframe\"]\n",
    "\n",
    "                # Insert data using the bulk_insert function\n",
    "                bulk_insert(table_name, df, conn)\n",
    "                end = time.time()\n",
    "                logging.info(f\"Insertion completed for table '{table_name}' in {end - start:.2f} seconds.\")\n",
    "                # insert_data(conn, table_name, data[\"dataframe\"], data[\"insert_sql\"])\n",
    "                # logging.info(f\"Insertion completed for table '{table_name}'.\")\n",
    "\n",
    "        # Commit all insertions as a single transaction\n",
    "        transaction.commit()\n",
    "        logging.info(\"All data insertions committed successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        # Rollback transaction if any insertion fails\n",
    "        logging.error(f\"Data insertion process failed, rolling back transaction: {e}\")\n",
    "        transaction.rollback()\n",
    "        raise  # Reraise the exception for further handling\n",
    "\n",
    "    finally:\n",
    "        if transaction.is_active:\n",
    "            logging.info(\"All data insertions committed successfully.\")\n",
    "            \n",
    "end_time = datetime.now()\n",
    "logging.info(f\"Pipeline load process completed at {end_time}. Duration: {end_time - start_time}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
